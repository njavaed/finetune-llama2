{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8e452-79b3-4beb-a62c-27bd542337ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "References\n",
    "https://github.com/edumunozsala/llama-2-7B-4bit-python-coder/blob/main/Llama-2-finetune-qlora-python-coder.ipynb\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8cb7dc9-efa5-449d-9415-cdb891d1861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proxy setup\n",
    "import os\n",
    "\n",
    "os.environ['http_proxy'] = 'http://internet.ford.com:83'\n",
    "os.environ['https_proxy'] = 'http://internet.ford.com:83'\n",
    "os.environ['no_proxy'] = '.ford.com,localhost,19.0.0.0/8,127.0.0.1,10.0.0.0/8,19.*'\n",
    "os.environ['HTTP_PROXY'] = 'http://internet.ford.com:83'\n",
    "os.environ['HTTPS_PROXY'] = 'http://internet.ford.com:83'\n",
    "os.environ['NO_PROXY'] = '.ford.com,localhost,19.0.0.0/8,127.0.0.1,10.0.0.0/8,19.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3101a7d-5bbf-4929-90a5-9be7a334263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/s/njavaed/ai-software-engineering/hf_cache/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/s/njavaed/ai-software-engineering/hf_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343829cb-8273-4a82-a700-0b174cab354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /s/njavaed/.local/lib/python3.8/site-packages (0.17.7)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.8/dist-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (4.25.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.12.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (72.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.17.7\n",
      "    Uninstalling wandb-0.17.7:\n",
      "      Successfully uninstalled wandb-0.17.7\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/s/njavaed/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed wandb-0.17.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977f6acd-e139-43df-bc35-66eb10b62f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10427dbe-f138-4ccf-b58e-4b3982a17063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "# model_id = \"codellama/CodeLlama-7b-hf\"\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "#dataset_name = \"HuggingFaceH4/CodeAlpaca_20K\"\n",
    "# Dataset split\n",
    "dataset_split= \"train\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"fine_tuned_model/codellama-7b-int4-python-code-18k\"\n",
    "# Huggingface repository\n",
    "#hf_model_repo=\"edumunozsala/\"+new_model\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0} #place all layers of model on the same gpu. To speed up training we can put different layers on different gpus\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "# Nested quantization, or double quantization, is a more complex form of quantization where the quantization process is applied twice. This can further reduce the size and increase the speed of the network, but it may also further reduce accuracy.\n",
    "use_double_nested_quant = False \n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension, controls the size of a smaller \"brain\" (a LoRA adapter) that you're adding to the main brain.\n",
    "lora_r = 64\n",
    "# Alpha parameter for LoRA scaling\n",
    "# Think of lora_alpha as a volume knob for how much the LoRA adapter's influence matters.\n",
    "# A higher lora_alpha means the adapter's \"voice\" is louder, making it learn faster but also potentially overfitting (getting too stuck on the training data).\n",
    "# 16 is a typical value, giving the adapter enough influence without being too overwhelming.\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "# Dropout is like randomly turning off some of the adapter's connections during training.\n",
    "# This helps prevent overfitting, as it forces the adapter to learn more general patterns instead of memorizing the training data exactly.\n",
    "# 0.1 means 10% of the connections are randomly turned off, a common value for regularizing models.\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = new_model\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "# Imagine the LLM is taking notes while learning. Gradient accumulation is like taking notes for multiple examples before making a change.\n",
    "gradient_accumulation_steps = 1 # 2\n",
    "# Enable gradient checkpointing\n",
    "# This is a memory-saving technique, like taking notes on a small piece of paper instead of a huge book.\n",
    "# It helps the LLM learn faster without running out of memory.\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "# Imagine the LLM is learning by taking big steps. Gradient clipping prevents the LLM from taking steps that are too big, which can make learning unstable.\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "# This is like how much the LLM adjusts its knowledge with each step. A higher learning rate means bigger adjustments, potentially faster learning but also more instability.\n",
    "learning_rate = 1.0e-5 #2e-4 \n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "# This is like gently pushing the LLM to simplify its knowledge, preventing it from getting too complex and overfitting.\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "# The optimizer is like the LLM's teacher, helping it learn effectively.\n",
    "# paged_adamw_32bit is a popular optimizer for large language models.\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "# This is like changing the LLM's learning speed during training.\n",
    "# cosine means the learning rate starts high, decreases gradually, and then increases again slightly at the end.\n",
    "lr_scheduler_type = \"cosine\" #\"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "# This is like letting the LLM practice a bit before starting to learn seriously.\n",
    "# The learning rate increases gradually for a small portion of the training.\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = False\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "# Disable tqdm\n",
    "# This disables a progress bar that shows how much of the training is finished.\n",
    "disable_tqdm= False\n",
    "\n",
    "################################################################################\n",
    "# SFTTrainer parameters\n",
    "################################################################################\n",
    "# Efficient training: SFT requires processing a lot of text data. These parameters help make training more efficient by controlling the length of sequences and maximizing the use of available processing power.\n",
    "# Model capacity: The max_seq_length influences the model's ability to handle longer and more complex inputs during training and inference.\n",
    "# Maximum sequence length to use\n",
    "# Sets the maximum number of tokens the LLM can process at once.\n",
    "max_seq_length = 2048 #None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "# Enables grouping multiple short examples into longer sequences to improve training efficiency.\n",
    "packing = True #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e46be1-2c7e-424d-aed0-8215c4822883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "df = pd.read_parquet('18k_added_train-00000-of-00001-8b6e212f3e1ece96.parquet', engine='pyarrow')\n",
    "dataset = Dataset.from_pandas(df, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a67b3a-7208-4bee-97e3-023b166a5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the instruction format for iamtarun/python_code_instructions_18k_alpaca\n",
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca81bd51-cae6-4a4d-8c38-be16bff997a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the type\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_use_double_quant=use_double_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e78da98b-3291-49ec-85ea-5b7db691596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/njavaed/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ff5df6c32a4902b60962e94b382b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\", quantization_config=bnb_config, use_cache = True, device_map=device_map)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"/s/njavaed/ai-software-engineering/hf_cache/models--NousResearch--Llama-2-7b-hf\", quantization_config=bnb_config, use_cache = True, device_map=device_map)\n",
    "\n",
    "# Imagine the LLM's brain is divided into multiple sections.\n",
    "# pretraining_tp tells you how many sections were used for parallel processing during the initial training.\n",
    "# A value of 1 means the entire brain was processed as a single unit, while a higher value would mean it was split into multiple sections for parallel computation.\n",
    "model.config.pretraining_tp = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c471ea44-428b-4668-9cac-b522e7fe2595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92339ff25ee9422ea00b93f8c067f39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37eb8da9848b4a0a8584c7560db0c97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aa7788145a4134ad3a7bdbcd1d8857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1295da9ad2184dfc93b872fe49608e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Safety: By default, the AutoTokenizer is cautious and won't load tokenizers that contain custom code from the Model Hub. This is a security measure to prevent malicious code from being executed.\n",
    "# Trust: Setting trust_remote_code=True overrides this default behavior and allows the tokenizer to load even if it contains custom code. This is typically done when you're confident about the source of the tokenizer and its code.\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf3c0fe-2667-4a00-86e9-4678280bc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Not necessary when using SFTTrainer\n",
    "# prepare model for training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0466ef66-66d7-4872-a4ce-617c2a7062ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    max_steps=max_steps,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    disable_tqdm=disable_tqdm,\n",
    "    seed=42,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"llama2-fine-tune-18k-added\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463903b9-5c30-4081-9598-66032256937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Memory footprint 3.829936128 GB\n",
      "Flops 41402.461323264 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Get the memory footprint for this model\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_id,\n",
    "        \"max_length\" : 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"path\": dataset\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "model_flops = (\n",
    "  model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(\"Memory footprint\", model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d198524-2a1b-4a62-bfa7-a384ee078bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/njavaed/.local/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=packing,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e9d7d0d-2673-4ac2-9f75-c96b5550472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "313afc69-2c5d-4aa7-b703-0dcef3f6350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnjavaed\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb setup\n",
    "import os\n",
    "\n",
    "os.environ['WANDB_BASE_URL'] = 'https://www.wandb.ford.com' # PROD\n",
    "os.environ[\"WANDB_API_KEY\"] = ''\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = '/s/njavaed/ai-software-engineering/ford-wandb-prod.pem' # change location to your .pem file whereever it's located\n",
    "os.environ[\"WANDB_PROJECT\"] = \"codellama-finetuned-njavaed\"\n",
    "# os.environ[\"WANDB_AGENT_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "# os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"wandb-working-training.ipynb\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"codellama-finetuned-njavaed\",\n",
    "    # Track hyperparameters and run metadata\n",
    "        # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"dataset\": \"18k_added_train-00000-of-00001-8b6e212f3e1ece96.parquet\",\n",
    "    \"epochs\": 1,\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a21a5035-aa64-4f4b-b219-9fb8ae7c6af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/njavaed/.local/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/s/njavaed/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='4654' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 564/4654 40:31 < 4:54:52, 0.23 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.826400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.596600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.599000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.563400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.598100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model in local\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18cd1b3c-1cdb-473d-a534-b39deb87be28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train/learning_rate</td><td>▁▃▄▆▇█████████████████</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.12</td></tr><tr><td>train/global_step</td><td>564</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>0.5979</td></tr><tr><td>train/total_flos</td><td>9.420869524783104e+16</td></tr><tr><td>train/train_loss</td><td>0.63472</td></tr><tr><td>train/train_runtime</td><td>2436.5988</td></tr><tr><td>train/train_samples_per_second</td><td>7.64</td></tr><tr><td>train/train_steps_per_second</td><td>1.91</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-mountain-17</strong> at: <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/nmg5k0lu' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/nmg5k0lu</a><br/> View project at: <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240813_193243-nmg5k0lu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Upgrade to the 0.57.2 version of W&B Server to get the latest features. Learn more: <a href='https://wandb.me/server-upgrade' target=\"_blank\">https://wandb.me/server-upgrade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48f194a-2f1a-4adc-8708-991845e033bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57397259-3492-4ff3-b43f-701c5b0636b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the trained and saved model and merge it then we can save the whole model\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = new_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc012bf0-21a1-44fa-a660-fff7356f54ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c28cc-128a-4a0f-a947-b4361a14cd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df611971-0b0e-4cd9-b0ef-d6854e334e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/s/njavaed/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
      "\n",
      "### Task:\n",
      "Create a Python function that takes a string and outputs the count of the number of uppercase letters in the string.\n",
      "\n",
      "### Input:\n",
      "‘Hello World’\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "def count_uppercase(string):\n",
      "    uppercase_count = 0\n",
      "    for letter in string:\n",
      "        if letter.isupper():\n",
      "            uppercase_count += 1\n",
      "    return uppercase_count\n",
      "\n",
      "print(count_uppercase('Hello World'))\n",
      "\n",
      "\n",
      "Ground truth:\n",
      "def countUppercase(myStr):\n",
      "    uppercaseCount = 0\n",
      "    for l in myStr:\n",
      "        if l.isupper():\n",
      "            uppercaseCount+=1\n",
      "    return uppercaseCount\n",
      "\n",
      "print(countUppercase('Hello World'))\n"
     ]
    }
   ],
   "source": [
    "# Test the merged model\n",
    "\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cf3d8bb-70e3-49f5-849f-3dc51ac3c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"instruction\": \"Given a nested fstring, find the sum of the integers in the fstring\", \"input\": \"n1  = 1, n2 = 2\", \n",
    "          \"output\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca23ef22-48fd-4307-a28c-a063c9b664ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
      "\n",
      "### Task:\n",
      "Given a nested fstring, find the sum of the integers in the fstring\n",
      "\n",
      "### Input:\n",
      "n1  = 1, n2 = 2\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "import sys\n",
      "\n",
      "def fstring_sum(n1, n2):\n",
      "    result = 0\n",
      "    for i in range(n1, n2+1):\n",
      "        result += i\n",
      "    return result\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    print(fstring_sum(1, 2))\n",
      "\n",
      "\n",
      "Ground truth:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7b3935-0615-473d-b734-3f8edfbd0504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def fstring_sum(n1, n2):\n",
    "    result = 0\n",
    "    for i in range(n1, n2+1):\n",
    "        result += i\n",
    "    return result\n",
    "\n",
    "print(fstring_sum(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9368c52f-7c2e-46a2-9ee5-3f78e187dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"instruction\": \"Generate a python script using PyGWalker from 2024 Python release\", \"input\": \"list_of_edges = [(1, 2), (2, 3), (3, 4), (4, 1)]\", \n",
    "          \"output\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f38429d4-1eb6-40b0-aed7-1420bd81e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "### Instruction:\n",
      "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
      "\n",
      "### Task:\n",
      "Generate a python script using PyGWalker from 2024 Python release\n",
      "\n",
      "### Input:\n",
      "list_of_edges = [(1, 2), (2, 3), (3, 4), (4, 1)]\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "\n",
      "Generated instruction:\n",
      "import pygwalker as pgw\n",
      "\n",
      "pgw.walk(list_of_edges)\n",
      "\n",
      "\n",
      "Ground truth:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Task below and the Input given to write the Response, which is a programming code that can solve the following Task:\n",
    "\n",
    "### Task:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25eaaedf-1478-4108-b94c-072b5323f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/s/njavaed/ai-software-engineering/wandb/run-20240826_213130-gdgbv6yc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/gdgbv6yc' target=\"_blank\">worldly-fog-19</a></strong> to <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/gdgbv6yc' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/gdgbv6yc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Artifact codellama-7b-int4-python-code-18k>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"codellama-finetuned-njavaed\",\n",
    "    # Track hyperparameters and run metadata\n",
    "        # track hyperparameters and run metadata\n",
    "    \n",
    "    )\n",
    "\n",
    "art = wandb.Artifact(\"codellama-7b-int4-python-code-18k\", type=\"model\")\n",
    "art.add_file(\"merged_model/model-00001-of-00002.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8deec1b6-5b52-447e-858f-64b6b86963c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact codellama-7b-int4-python-code-18k>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51185cfc-4fd5-482d-9897-b7b6d88a2bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gdgbv6yc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.048 MB of 9514.455 MB uploaded\\r'), FloatProgress(value=5.04889963236871e-06, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-fog-19</strong> at: <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/gdgbv6yc' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/gdgbv6yc</a><br/> View project at: <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed</a><br/>Synced 4 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240826_213130-gdgbv6yc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Upgrade to the 0.57.2 version of W&B Server to get the latest features. Learn more: <a href='https://wandb.me/server-upgrade' target=\"_blank\">https://wandb.me/server-upgrade</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gdgbv6yc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1ce8c2fb85474faa1a92ab6b0d5b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112279196580251, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/s/njavaed/ai-software-engineering/wandb/run-20240826_213357-1vxs7icx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/1vxs7icx' target=\"_blank\">jumping-sound-20</a></strong> to <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/1vxs7icx' target=\"_blank\">https://www.wandb.ford.com/njavaed/codellama-finetuned-njavaed/runs/1vxs7icx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Artifact codellama-7b-int4-python-code-18k>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(entity='njavaed', project='codellama-finetuned-njavaed')\n",
    "art = wandb.Artifact('codellama-7b-int4-python-code-18k', type='model')\n",
    "# ... add content to artifact ...\n",
    "wandb.log_artifact(art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd9b46-e602-4be3-8e21-20a64c29c6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
